{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Fast Weights to Attend to the Recent Past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 바벨피쉬 / 바벨보이스 : 파트 1 - 딥NLP [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* Abstract\n",
    "* 1 Introduction\n",
    "* 2 Evidence from physiology that temporary memory may not be stored as neural activities\n",
    "    - 3 Fast Associative Memory\n",
    "    - 3.1 Layer normalized fast weights\n",
    "* 4 Experimental results\n",
    "    - 4.1 Associative retrieval\n",
    "    - 4.2 Integrating glimpses in visual attention models\n",
    "    - 4.3 Facial expression recognition\n",
    "    - 4.4 Agents with memory\n",
    "* 5 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "#### restriction\n",
    "* Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: \n",
    "    - Neural activities that \n",
    "        - represent the current or recent input and\n",
    "    - weights that \n",
    "        - learn to capture regularities among inputs, outputs and payoffs. \n",
    "       \n",
    "#### different time-scales\n",
    "* Synapses \n",
    "    - have dynamics at many different time-scales and \n",
    "* this suggests that artificial neural networks might benefit \n",
    "    - from variables that change <font color=\"red\">slower than activities</font> but \n",
    "    - much <font color=\"red\">faster than the standard weights</font>. \n",
    "\n",
    "#### fast weights\n",
    "* These “fast weights” can be used to \n",
    "    - <font color=\"red\">store temporary memories of the recent past</font> and \n",
    "* they provide a neurally plausible way of \n",
    "    - <font color=\"red\">implementing the type of attention</font> to the past \n",
    "        - that has recently proved very helpful in sequence-to-sequence models. \n",
    "* By using fast weights \n",
    "    - we can <font color=\"red\">avoid</font> \n",
    "        - the <font color=\"red\">need to store copies</font> \n",
    "            - of <font color=\"red\">neural activity patterns</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN's memory capacitiy\n",
    "* Ordinary recurrent neural networks typically have \n",
    "    - two types of memory that \n",
    "        - have very different time scales, \n",
    "        - very different capacities and \n",
    "        - very different computational roles. \n",
    "* The history of the sequence currently being processed is \n",
    "    - stored \n",
    "        - in the hidden activity vector, \n",
    "            - which acts as \n",
    "                - a short-term memory that is \n",
    "                    - updated at every time step. \n",
    "* The capacity of this memory is\n",
    "    - <font color=\"red\">O(H)</font> \n",
    "        - where H is the number of hidden units. \n",
    "    - Long-term memory (matrices)\n",
    "        - about \n",
    "            - how to convert the current input and hidden vectors \n",
    "                - into the next hidden vector and \n",
    "            - a predicted output vector \n",
    "                - is stored \n",
    "                    - in the weight matrices \n",
    "                        - connecting \n",
    "                            - the hidden units \n",
    "                                - to themselves and \n",
    "                            - to the inputs and outputs. \n",
    "        - These matrices are \n",
    "            - typically updated \n",
    "                - at the end of a sequence and \n",
    "            - their capacity is \n",
    "                - <font color=\"red\">O(H2) + O(IH) + O(HO)</font> \n",
    "                    - where I and O are the numbers of input and output units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "* LSTMs are still limited \n",
    "    - to a short-term memory capacity of \n",
    "        - <font color=\"red\">O(H)</font> \n",
    "            - for the history of the current sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### thired form of memory (fast weights)\n",
    "* Several researchers [Hinton and Plaut, 1987, Schmidhuber, 1992] have suggested that \n",
    "    - neural networks could benefit \n",
    "        - from a third form of memory that \n",
    "            - has <font color=\"red\">much higher storage capacity</font> \n",
    "                - than <font color=\"blue\">the neural activities</font> but \n",
    "            - <font color=\"red\">much faster dynamics</font>\n",
    "                - than <font color=\"blue\">the standard “slow” weights</font>\n",
    "* This memory could \n",
    "    - store \n",
    "        - information \n",
    "            - specific to the history of the current sequence\n",
    "    - so that this information is \n",
    "        - available \n",
    "            - to <font color=\"red\">influence</font> \n",
    "                - the <font color=\"red\">ongoing processing</font> \n",
    "                    - <font color=\"blue\">without</font> \n",
    "                        - using up the memory capacity of \n",
    "                            - the <font color=\"blue\">hidden activities</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until recently, however, there was surprisingly little investigation of other forms of memory in recurrent nets despite strong psychological evidence that it exists and obvious computational reasons why it was needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Evidence from physiology that temporary memory may not be stored as neural activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The brain implements a variety of short-term plasticity mechanisms that operate on intermediate timescale.\n",
    "* These plasticity mechanisms are all synapse-specific. Thus they are more accurately modeled by a memory with O(H2) capacity than the O(H) of standard recurrent artificial recurrent neural nets and LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Fast Associative Memory\n",
    "* 3.1 Layer normalized fast weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main preoccupations of neural network research in the 1970s and early 1980s [Willshaw et al., 1969, Kohonen, 1972, Anderson and Hinton, 1981, Hopfield, 1982] was the idea that <font color=\"red\">memories were not stored by somehow keeping copies of patterns of neural activity</font>. <font color=\"blue\">Instead, these patterns were reconstructed when needed from information stored in the weights of an associative network</font> and \n",
    "* the very same weights could store many different memories An auto-associative memory that\n",
    "    - has $N^2$ weights cannot be expected to store more that \n",
    "        - N real-valued vectors \n",
    "            - with N components each. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">A fast associative memory has several advantages</font> when compared with the type of memory assumed by a Neural Turing Machine (NTM) [Graves et al., 2014], Neural Stack [Grefenstette et al., 2015], or Memory Network [Weston et al., 2014]. \n",
    "* First, \n",
    "    - it is not at all clear how a real brain would implement the more exotic structures in these models e.g., the tape of the NTM, \n",
    "    - whereas it is clear that the brain could \n",
    "        - implement a fast associative memory in synapses with the appropriate dynamics. \n",
    "* Second, \n",
    "    - in a fast associative memory \n",
    "        - there is no need to decide \n",
    "            - where or when to write to memory and \n",
    "            - where or when to read from memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time the input changes there is a transition to a new hidden state which is determined by a <font color=\"red\">combination of three sources of information</font> :\n",
    "* The new input via the slow input-to-hidden weights, \n",
    "    - C, \n",
    "* the previous hidden state via the slow transition weights, \n",
    "    - W , and \n",
    "* the recent history of hidden state vectors via the fast weights, \n",
    "    - A.\n",
    "* The effect of the first two sources of information \n",
    "    - on the new hidden state \n",
    "    - can be computed once and \n",
    "    - then maintained \n",
    "        - as a <font color=\"red\">sustained boundary condition</font> \n",
    "            - for a brief iterative settling process \n",
    "                - which allows the fast weights \n",
    "                    - to influence the new hidden state.\n",
    "* <font color=\"blue\">Assuming that the fast weights decay exponentially</font>, \n",
    "    - we now show that \n",
    "        - the effect of the fast weights \n",
    "            - on the hidden vector \n",
    "            - during an iterative settling phase \n",
    "        - is to provide an additional input \n",
    "            - that is proportional to the sum over all recent hidden activity vectors \n",
    "                - of the scalar product of that recent hidden vector \n",
    "                    - with the current hidden activity vector, \n",
    "                        - with each term in this sum being weighted \n",
    "                            - by the decay rate raised to the power of how long ago that hidden vector occurred.\n",
    "* So fast weights \n",
    "    - <font color=\"blue\">act like a kind of attention to the recent past</font> \n",
    "    - but with the <font color=\"blue\">strength of the attention</font> \n",
    "        - being <font color=\"blue\">determined</font> \n",
    "            - by the <font color=\"blue\">scalar product</font> \n",
    "                - between the <font color=\"blue\">current hidden vector and the earlier hidden vector</font> \n",
    "                    - rather than being determined by a separate parameterized computation of the type used in neural machine translation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update rule : fast memory weight matrix. A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule for the fast memory weight matrix, A, is simply to multiply the current fast weights by a decay rate, λ, and add the outer product of the hidden state vector, h(t), multiplied by a learning\n",
    "rate, η:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next vector of hidden activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next vector of hidden activities, h(t + 1), is computed in two steps.\n",
    "* The “preliminary” vector h0(t + 1) \n",
    "    - is determined \n",
    "        - by the combined effects of \n",
    "            - the input vector x(t) and \n",
    "            - the previous hidden vector: \n",
    "                - h0(t + 1) = f(Wh(t) + Cx(t)), \n",
    "                    - where W and C are slow weight matrices and \n",
    "                    - f(.) is the nonlinearity used by the hidden units. \n",
    "* The preliminary vector is then used to initiate an “inner loop” iterative process \n",
    "    - which runs for S steps and \n",
    "    - progressively changes the hidden state into \n",
    "        - h(t+1) = hS(t+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the terms in square brackets are the sustained boundary conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming A is 0 at the beginning of the sequence,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term in square brackets is just the scalar product of an earlier hidden state vector, h(τ ), with the current hidden state vector, hs(t+1), during the iterative inner loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inner loop\n",
    "* So at each iteration of the inner loop, \n",
    "    - the fast weight matrix is \n",
    "        - <font color=\"red\">exactly equivalent to attending to past hidden vectors</font> \n",
    "            - in proportion to their scalar product \n",
    "                - with the current hidden vector, \n",
    "                - weighted by a decay factor.\n",
    "* During the inner loop iterations, \n",
    "    - attention will become \n",
    "        - more focussed on \n",
    "            - past hidden states that \n",
    "                - manage to attract the current hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Layer normalized fast weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A potential problem with fast associative memory is that \n",
    "    - the scalar product of two hidden vectors could \n",
    "        - vanish or explode \n",
    "            - depending on \n",
    "                - the norm of the hidden vectors. \n",
    "* Recently, layer normalization [Ba et al., 2016] has been shown to be very effective at stablizing the hidden state dynamics in RNNs and reducing training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We found that applying layer normalization on each iteration of the inner loop makes the fast associative memory more robust to the choice of learning rate and decay hyper-parameters.\n",
    "* For the rest of the paper, fast weight models are trained using layer normalization and the outer product learning rule with fast learning rate of 0.5 and decay rate of 0.95, unless otherwise noted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Experimental results\n",
    "* 4.1 Associative retrieval\n",
    "* 4.2 Integrating glimpses in visual attention models\n",
    "* 4.3 Facial expression recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Associative retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where ‘?’ is the token to separate the query from the key-value pairs.\n",
    "\n",
    "We generated 100,000 training examples, 10,000 validation examples and 20,000 test examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 and Table 1 show that when the number of recurrent units is small, the fast associative memory significantly outperforms the LSTMs with the same number of recurrent units. The result fits with our hypothesis that the fast associative memory allows the RNN to use its recurrent units more effectively. In addition to having higher retrieval accuracy, the model with fast weights also converges faster than the LSTM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Integrating glimpses in visual attention models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Facial expression recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Agents with memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] (paper) Using Fast Weights to Attend to the Recent Past\n",
    "* [2] (code) ajarai/fast-weights - https://github.com/ajarai/fast-weights\n",
    "* [3] https://theneuralperspective.com/2016/12/04/implementation-of-using-fast-weights-to-attend-to-the-recent-past/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
